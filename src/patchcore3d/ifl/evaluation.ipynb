{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from deli import save, save_json\n",
    "\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_network import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 10)\n",
    "matplotlib.rc('image', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments settings and snapshot\n",
    "experiment_directory = '../data/ifl_experiment/'\n",
    "# experiment_directory = './experiments/hcp_pxfull_k10_mp2/'\n",
    "continue_from = 'latest'\n",
    "\n",
    "device = torch.device( 'cuda:0' ) if torch.cuda.is_available() else torch.device( 'cpu' )\n",
    "# device = torch.device( 'cpu' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model from *experiment_directory* settings and load saved state dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_filename = os.path.join(experiment_directory, \"specs.json\")\n",
    "\n",
    "if not os.path.isfile(specs_filename):\n",
    "    raise Exception(\n",
    "        'The experiment directory does not include specifications file \"specs.json\"'\n",
    "    )\n",
    "\n",
    "specs = json.load(open(specs_filename))\n",
    "arch = __import__(\"network\", fromlist=[\"Decoder\"])\n",
    "latent_size = specs[\"CodeLength\"]\n",
    "decoder = arch.Decoder(latent_size, **specs[\"NetworkSpecs\"])\n",
    "\n",
    "# decoder = torch.nn.DataParallel(decoder)\n",
    "\n",
    "saved_model_state = torch.load(\n",
    "    os.path.join(\n",
    "        experiment_directory, ws.model_params_subdir, continue_from + \".pth\"\n",
    "    )\n",
    ")\n",
    "saved_model_epoch = saved_model_state[\"epoch\"]\n",
    "decoder.load_state_dict(saved_model_state[\"model_state_dict\"], strict=False)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data loader and load latent vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_bound = get_spec_with_default(specs, \"CodeBound\", None)\n",
    "\n",
    "\n",
    "## Obtain the embeddings from a new experiment\n",
    "\n",
    "# data_source = specs[\"DataSource\"]\n",
    "# scenes_per_batch = specs[\"ScenesPerBatch\"]\n",
    "# samples_per_scene = specs[\"SamplesPerScene\"]\n",
    "\n",
    "# num_data_loader_threads = get_spec_with_default(specs, \"DataLoaderThreads\", 1)\n",
    "\n",
    "# loader = preprocessing.xyzs_loader(data_dir = data_source,\n",
    "#                                    scenes_per_batch = scenes_per_batch,\n",
    "#                                    samples_per_scene = samples_per_scene,\n",
    "#                                    num_workers = num_data_loader_threads\n",
    "#                                    )\n",
    "\n",
    "# num_scenes = len(loader.data)\n",
    "\n",
    "# lat_vecs = torch.nn.Embedding(num_scenes, latent_size, max_norm=code_bound).to(device)\n",
    "\n",
    "\n",
    "\n",
    "## Or use the 1055 embeddings with Brain MRIs from HCP provided in github\n",
    "lat_vecs = torch.nn.Embedding(1055, latent_size, max_norm=code_bound).to(device)\n",
    "\n",
    "\n",
    "\n",
    "lat_epoch = load_latent_vectors(\n",
    "            experiment_directory, continue_from + \".pth\", lat_vecs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate samples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a specific latent vector from training set [0,1055]\n",
    "\n",
    "# i = torch.tensor([123]).to(device)\n",
    "# features = lat_vecs(i)\n",
    "\n",
    "\n",
    "## Or randomly sample from N(0,I)\n",
    "\n",
    "features = torch.randn(specs[\"CodeLength\"]).to(device) / 10\n",
    "\n",
    "\n",
    "\n",
    "# Generate a grid with given resolution (central axial slide retreieve)\n",
    "resolution = 200\n",
    "\n",
    "y = torch.linspace(-1,1,resolution)\n",
    "z = torch.linspace(-1,1,resolution)\n",
    "grid_y, grid_z = torch.meshgrid(y,z)\n",
    "grid_yz = torch.stack((grid_y,grid_z)).permute(1,2,0).reshape(-1,2)\n",
    "\n",
    "# Append 0s to get central axial slide\n",
    "grid_xyz = torch.cat((torch.zeros((grid_yz.shape[0],1)),grid_yz),dim=1)\n",
    "grid_xyz = grid_xyz.to(device)\n",
    "\n",
    "# Retrieve network outputs for the coordinate grid\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    input = torch.cat([features.expand(grid_xyz.shape[0],-1), grid_xyz], dim=1)\n",
    "    out = decoder(input)\n",
    "\n",
    "# Get argmax to plot intensity clusters\n",
    "out_argmax = out[:,:-1].argmax(1)\n",
    "out_argmax = out_argmax.reshape(resolution,resolution)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(out_argmax.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For test dataset, obtain latent representations and calculate Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define functions to retrieve z given image X using gradient descent and a function to get X given a z\n",
    "\n",
    "def get_lat_vecs(img, samples,iterations, lat_vecs = None):\n",
    "    ''' Retrieves optimal latent vector z for img, using backpropagation to move along latent space'''\n",
    "    decoder.eval()\n",
    "    XYZ = img.shape\n",
    "    XYZ_t = torch.tensor(XYZ).unsqueeze(0).to(device)\n",
    "    \n",
    "    # If latent vectors are not passed, initialize randomly\n",
    "    if lat_vecs is None:\n",
    "        # initialize lat_vecs randomly and define loss fn and optimizer\n",
    "        lat_vecs = torch.nn.Embedding(1, specs['CodeLength'], max_norm=specs['CodeBound']).to(device)\n",
    "        torch.nn.init.normal_(\n",
    "            lat_vecs.weight.data,\n",
    "            0.0,\n",
    "            get_spec_with_default(specs, \"CodeInitStdDev\", 1.0) / math.sqrt(latent_size),\n",
    "            )\n",
    "\n",
    "    # Define optimizer and loss\n",
    "    optimizer_lat_vecs = torch.optim.Adam(lat_vecs.parameters(),)\n",
    "    loss_fn_class = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # Sample K points, retrieve intensities for points sample and normalize coordinates\n",
    "        xyz = [random.choices(range(size), k=samples) for size in XYZ]\n",
    "        s = torch.from_numpy(img[tuple(xyz)]).to(device)\n",
    "\n",
    "        xyz = torch.tensor(xyz).T.float().to(device)\n",
    "        xyz /= XYZ_t\n",
    "        xyz = (xyz * 2) - 1\n",
    "\n",
    "        # Pass samples through network, calculate loss and apply backpropagation to latent vectors\n",
    "        optimizer_lat_vecs.zero_grad()\n",
    "        input = torch.cat([lat_vecs.weight.expand(xyz.shape[0],-1), xyz], dim=1)\n",
    "        net_out = decoder(input)\n",
    "\n",
    "        pred_logp_class = net_out\n",
    "\n",
    "        batch_loss = loss_fn_class(pred_logp_class, s) / samples\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer_lat_vecs.step()\n",
    "    \n",
    "    return lat_vecs\n",
    "\n",
    "def get_predictions(lat_vec, XYZ):\n",
    "    '''Retrieves dense image with dimensionality XYZ from the model for lat_vec (z)'''\n",
    "    x = torch.linspace(-1,1,XYZ[0])\n",
    "    y = torch.linspace(-1,1,XYZ[1])\n",
    "    z = torch.linspace(-1,1,XYZ[2])\n",
    "    grid_x, grid_y, grid_z = torch.meshgrid(x,y,z)\n",
    "    grid_xyz = torch.stack((grid_x,grid_y,grid_z)).permute(1,2,3,0).reshape(XYZ[0],-1,3)\n",
    "    \n",
    "    decoder.eval()\n",
    "    recon_xyz = []\n",
    "    \n",
    "    for batch_xyz in grid_xyz:\n",
    "        with torch.no_grad():\n",
    "            input = torch.cat([lat_vec.weight.expand(batch_xyz.shape[0],-1), batch_xyz.to(device)], dim=1)\n",
    "            out = decoder(input)\n",
    "            recon_xyz.append(out.cpu())            \n",
    "\n",
    "    recon_xyz = torch.stack(recon_xyz)\n",
    "    recon_xyz = recon_xyz.reshape(XYZ+(-1,))\n",
    "    \n",
    "    return recon_xyz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define test loader (retrieves the full volume and ground truth segmentation mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify folder where evaluation / test folder sits\n",
    "data_path = '../data/ifl_ixi_preproc_test/'\n",
    "vol_loader = preprocessing.vol_loader(data_path,\n",
    "                                      samples=1,\n",
    "                                      num_workers=1)\n",
    "\n",
    "# Get a list of case ids to evaluate\n",
    "# list_evaluate = ['Brats18_2013_5_1','Brats18_CBICA_AOZ_1','Brats18_CBICA_ATP_1','Brats18_CBICA_AUR_1',\n",
    "#              'Brats18_TCIA02_309_1','Brats18_TCIA08_105_1','Brats18_TCIA09_141_1','Brats18_TCIA10_387_1']\n",
    "list_evaluate = [fname[:-3] for fname in os.listdir(data_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(experiment_directory, 'preds'), exist_ok=True)\n",
    "\n",
    "fs = 15\n",
    "smooth_op = torch.nn.Sequential(torch.nn.MaxPool3d(kernel_size=3,stride=1,padding=1),\n",
    "                                torch.nn.AvgPool3d(kernel_size=fs,stride=1,padding=fs//2),)\n",
    "target_size_small = (70, 70, 45)\n",
    "times = []\n",
    "\n",
    "for i, img_batch in enumerate(tqdm(vol_loader)):\n",
    "\n",
    "    if img_batch.cid[0] in list_evaluate:\n",
    "\n",
    "#         print(str(i)+\" - \"+img_batch.cid[0])\n",
    "        start_time = time.time()\n",
    "        img = img_batch.img[0].astype(int)\n",
    "        seg = img_batch.seg[0]\n",
    "        XYZ = img.shape\n",
    "        \n",
    "        # Get z and X'\n",
    "        lat_vecs = get_lat_vecs(img,samples=10000,iterations=501)\n",
    "        lat_vecs = get_lat_vecs(img,samples=50000,iterations=101, lat_vecs=lat_vecs)\n",
    "\n",
    "        recon_xyz = get_predictions(lat_vecs, XYZ)\n",
    "        \n",
    "        # Calculate cross-entropy\n",
    "        gt = torch.from_numpy(img).unsqueeze(0)\n",
    "        xent = torch.nn.functional.cross_entropy(recon_xyz.unsqueeze(0).permute(0,4,1,2,3),gt,reduction='none')\n",
    "        \n",
    "        # apply smoothing\n",
    "        err_smooth = -smooth_op(-xent).squeeze().cpu()\n",
    "                \n",
    "        err_smooth = np.pad(err_smooth, [(0, 0), (5, 6), (0, 0)], 'constant', constant_values = 0)\n",
    "        err_smooth = err_smooth[20:-20, :, 20:-20]\n",
    "        err_smooth = err_smooth[:, ::-1, :]\n",
    "        err_smooth = err_smooth.transpose( (2, 1, 0) )\n",
    "        \n",
    "        times.append(time.time() - start_time)\n",
    "#         print(\"--- %s seconds ---\" % (times[-1]))\n",
    "\n",
    "        # Save cross-entropy, segmentations and case ids\n",
    "        small_prediction = F.interpolate(\n",
    "                torch.tensor(err_smooth.copy())[None, None, ...], size=target_size_small, mode=\"trilinear\", align_corners=False\n",
    "            )[0, 0].numpy()\n",
    "        save(small_prediction, os.path.join(experiment_directory, 'preds', f'{img_batch.cid[0]}.npy.gz'), compression=1)\n",
    "\n",
    "save_json(times, os.path.join(experiment_directory, 'times.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify folder where evaluation / test folder sits\n",
    "data_path = '../data/ifl_brats_preproc/'\n",
    "vol_loader = preprocessing.vol_loader(data_path,\n",
    "                                      samples=1,\n",
    "                                      num_workers=1)\n",
    "\n",
    "# Get a list of case ids to evaluate\n",
    "# list_evaluate = ['Brats18_2013_5_1','Brats18_CBICA_AOZ_1','Brats18_CBICA_ATP_1','Brats18_CBICA_AUR_1',\n",
    "#              'Brats18_TCIA02_309_1','Brats18_TCIA08_105_1','Brats18_TCIA09_141_1','Brats18_TCIA10_387_1']\n",
    "list_evaluate = [fname[:-3] for fname in os.listdir(data_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(experiment_directory, 'preds'), exist_ok=True)\n",
    "\n",
    "fs = 15\n",
    "smooth_op = torch.nn.Sequential(torch.nn.MaxPool3d(kernel_size=3,stride=1,padding=1),\n",
    "                                torch.nn.AvgPool3d(kernel_size=fs,stride=1,padding=fs//2),)\n",
    "target_size_small = (70, 70, 45)\n",
    "times = []\n",
    "\n",
    "for i, img_batch in enumerate(tqdm(vol_loader)):\n",
    "\n",
    "    if img_batch.cid[0] in list_evaluate:\n",
    "\n",
    "#         print(str(i)+\" - \"+img_batch.cid[0])\n",
    "        start_time = time.time()\n",
    "        img = img_batch.img[0].astype(int)\n",
    "        seg = img_batch.seg[0]\n",
    "        XYZ = img.shape\n",
    "        \n",
    "        # Get z and X'\n",
    "        lat_vecs = get_lat_vecs(img,samples=10000,iterations=501)\n",
    "        lat_vecs = get_lat_vecs(img,samples=50000,iterations=101, lat_vecs=lat_vecs)\n",
    "\n",
    "        recon_xyz = get_predictions(lat_vecs, XYZ)\n",
    "        \n",
    "        # Calculate cross-entropy\n",
    "        gt = torch.from_numpy(img).unsqueeze(0)\n",
    "        xent = torch.nn.functional.cross_entropy(recon_xyz.unsqueeze(0).permute(0,4,1,2,3),gt,reduction='none')\n",
    "        \n",
    "        # apply smoothing\n",
    "        err_smooth = -smooth_op(-xent).squeeze().cpu()\n",
    "                \n",
    "        err_smooth = np.pad(err_smooth, [(0, 0), (5, 6), (0, 0)], 'constant', constant_values = 0)\n",
    "        err_smooth = err_smooth[20:-20, :, 20:-20]\n",
    "        err_smooth = err_smooth[:, ::-1, :]\n",
    "        err_smooth = err_smooth.transpose( (2, 1, 0) )\n",
    "        \n",
    "        times.append(time.time() - start_time)\n",
    "#         print(\"--- %s seconds ---\" % (times[-1]))\n",
    "\n",
    "        # Save cross-entropy, segmentations and case ids\n",
    "        small_prediction = F.interpolate(\n",
    "                torch.tensor(err_smooth.copy())[None, None, ...], size=target_size_small, mode=\"trilinear\", align_corners=False\n",
    "            )[0, 0].numpy()\n",
    "        save(small_prediction, os.path.join(experiment_directory, 'preds', f'{img_batch.cid[0]}.npy.gz'), compression=1)\n",
    "\n",
    "save_json(times, os.path.join(experiment_directory, 'times_brats.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-processing of the anomaly score, consist of a MinPool layer [Implemented as -MaxPool(-X)] and an AvgPool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = 15\n",
    "smooth_op = torch.nn.Sequential(torch.nn.MaxPool3d(kernel_size=3,stride=1,padding=1),\n",
    "                                torch.nn.AvgPool3d(kernel_size=fs,stride=1,padding=fs//2),)\n",
    "\n",
    "errors_smooth = []\n",
    "for i in errors_all:\n",
    "    i = i.unsqueeze(0).to(device)\n",
    "    i_smooth = -smooth_op(-i).squeeze().cpu()\n",
    "    errors_smooth.append(i_smooth)\n",
    "    \n",
    "errors_smooth = torch.stack(errors_smooth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get evaluation metrics for the Anomaly Scores. Note that DSC printed in the first cell is the max for all the curve, in test set it is needed to specify *thr* variable with the optimal threshold from the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(errors,segs,thr=None,dim=()):\n",
    "    \n",
    "    if thr is not None:\n",
    "        f1 = 2 * torch.sum(segs * (errors>thr),dim=dim).to(torch.float32)\\\n",
    "            / (torch.sum(segs,dim=dim)+torch.sum(errors>thr,dim=dim))\n",
    "        \n",
    "        return f1, None\n",
    "    else:\n",
    "        prec,rec,thrs = metrics.precision_recall_curve(segs.numpy().reshape(-1),\n",
    "                                       errors.numpy().reshape(-1),\n",
    "                                       pos_label=1)\n",
    "        \n",
    "\n",
    "        f1 = 2 * prec * rec /(prec+rec)\n",
    "        \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(segs.numpy().reshape(-1), errors.numpy().reshape(-1))\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "        \n",
    "        ap = metrics.average_precision_score(segs.numpy().reshape(-1),errors.numpy().reshape(-1))\n",
    "        \n",
    "        return (f1, auc,fpr[tpr>.95].min(),ap), thrs[f1.argmax()]\n",
    "    \n",
    "(f1,auc,FPR95,ap),thr = scores(errors_smooth,segs_all)\n",
    "\n",
    "print(\"ap: \",ap)\n",
    "print(\"rocauc: \",auc)\n",
    "print(\"FPR @95 Recall: \",FPR95)\n",
    "print(\"DSC: \",np.nanmax(f1))\n",
    "print(\"Threshold: \",thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_thr = thr\n",
    "f1, _ = scores(errors_smooth,segs_all,thr=val_thr,dim=(1,2,3))\n",
    "\n",
    "print(\"DSC by case: \",f1)\n",
    "\n",
    "print(\"DSC mean: \",f1.mean())\n",
    "print(\"DSC std: \",f1.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (30, 15)\n",
    "matplotlib.rc('image', cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_idx = {code:i for i,code in enumerate(list_evaluate)}\n",
    "idxs = np.array([ code_to_idx[i] for i in list_evaluate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print original images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 4\n",
    "fig, ax = plt.subplots(2,cols)\n",
    "\n",
    "for i,a in enumerate(ax):\n",
    "    for j,b in enumerate(a):\n",
    "\n",
    "        # Reload original image (pre-processed) \n",
    "        img = [img.img for img in vol_loader if img.cid[0] == cids[i*cols+j]][0]\n",
    "        \n",
    "        b.title.set_text(list_evaluate[i*cols+j])\n",
    "        b.tick_params(axis='both',which='both',top=False,bottom=False,left=False,right=False, \n",
    "                      labelbottom=False, labelleft=False)\n",
    "        \n",
    "        b.imshow(img[0,80])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print ground truth and segmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .8\n",
    "cmap = plt.cm.get_cmap('Paired')\n",
    "slice_no = 80\n",
    "\n",
    "def merge_segmentations(anomaly_segmentation, ground_truth):\n",
    "    \n",
    "    img = (ground_truth == 0) * 1.\n",
    "    \n",
    "    c1 = torch.tensor(cmap(1)[:-1]).view(1,-1,1,1).float()\n",
    "    c2 = torch.tensor(cmap(4)[:-1]).view(1,-1,1,1).float()    \n",
    "    \n",
    "    img = torch.where(anomaly_segmentation > 0, \n",
    "                      img * (1 - alpha) + alpha * c2,\n",
    "                      img)\n",
    "\n",
    "    return img.squeeze().permute(1,2,0)\n",
    "\n",
    "\n",
    "cols = 4\n",
    "fig, ax = plt.subplots(2,cols)\n",
    "\n",
    "for i,a in enumerate(ax):\n",
    "    for j,b in enumerate(a):\n",
    "        viz = merge_segmentations(errors_smooth[idxs[i*cols+j],slice_no]>val_thr,\n",
    "                                 segs_all[idxs[i*cols+j],slice_no])\n",
    "        \n",
    "        b.title.set_text(list_evaluate[i*cols+j])\n",
    "        b.tick_params(axis='both',which='both',top=False,bottom=False,left=False,right=False, \n",
    "                      labelbottom=False, labelleft=False)\n",
    "        \n",
    "        b.imshow(viz)\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
